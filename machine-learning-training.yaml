# This is a modified version of a workflow authored by Yashod Perera, if you found it useful, go check the original article
# in https://yashodgayashan.medium.com/from-notebooks-to-pipelines-supercharge-your-mlops-with-argo-on-kubernetes-73ef8d4d586f
# and support his work :-)

apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-pipeline-
spec:
  entrypoint: ml-workflow
  volumeClaimTemplates:
    - metadata:
        name: data-volume
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi
  arguments:
    parameters:
      - name: iteration
        value: "0"
  templates:
    - name: ml-workflow
      dag:
        tasks:
          - name: fetch-data
            template: fetch-data
            when: "{{workflow.parameters.iteration}} == 0"
          - name: data-preprocessing
            template: data-preprocess
            dependencies: [fetch-data]
            when: "{{workflow.parameters.iteration}} == 0"
          - name: model-training
            template: train-model
            dependencies: [data-preprocessing]
          - name: model-evaluation
            template: evaluate-model
            dependencies: [model-training]
          - name: accuracy-check
            template: check-accuracy
            dependencies: [model-evaluation]
            arguments:
              parameters:
                - name: test-accuracy
                  value: "{{tasks.model-evaluation.outputs.parameters.test-accuracy}}"
                - name: iteration
                  value: "{{workflow.parameters.iteration}}"

    # Step 0: Fetch input CSV from MinIO
    - name: fetch-data
      container:
        image: minio/mc
        envFrom:
          - secretRef:
              name: minio-secret
        command: ["sh", "-c"]
        args:
          - |
            mc alias set myminio "$MINIO_SERVER_HOST" "$MINIO_SERVER_ACCESS_KEY" "$MINIO_SERVER_SECRET_KEY"
            mc cp myminio/yashod-ml/input.csv /mnt/data/input.csv
        volumeMounts:
          - name: data-volume
            mountPath: /mnt/data

    # Step 1: Preprocess data
    - name: data-preprocess
      container:
        image: bitnami/python:latest
        command: ["sh", "-c"]
        args:
          - |
            pip install --no-cache-dir pandas && \
            python - <<'EOF'
            import pandas as pd
            df = pd.read_csv('/mnt/data/input.csv')
            df_cleaned = df.dropna()
            df_cleaned.to_csv('/mnt/data/cleaned_data.csv', index=False)
            EOF
        volumeMounts:
          - name: data-volume
            mountPath: /mnt/data

    # Step 2: Train model
    - name: train-model
      container:
        image: bitnami/python:latest
        command: ["sh", "-c"]
        args:
          - |
            pip install --no-cache-dir pandas scikit-learn joblib && \
            python - <<'EOF'
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            import joblib
            
            df = pd.read_csv('/mnt/data/cleaned_data.csv')
            X = df.drop('target', axis=1)
            y = df['target']
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
            model = RandomForestClassifier()
            model.fit(X_train, y_train)
            joblib.dump(model, '/mnt/data/model.joblib')
            EOF
        volumeMounts:
          - name: data-volume
            mountPath: /mnt/data

    # Step 3: Evaluate model
    - name: evaluate-model
      outputs:
        parameters:
          - name: test-accuracy
            valueFrom:
              path: /mnt/data/test-accuracy
      container:
        image: bitnami/python:latest
        command: ["sh", "-c"]
        args:
          - |
            pip install --no-cache-dir pandas scikit-learn joblib && \
            python - <<'EOF'
            import pandas as pd
            import joblib
            from sklearn.metrics import accuracy_score
            
            model = joblib.load('/mnt/data/model.joblib')
            df = pd.read_csv('/mnt/data/cleaned_data.csv')
            X = df.drop('target', axis=1)
            y = df['target']
      
            predictions = model.predict(X)
            accuracy = accuracy_score(y, predictions)
            print(f"Model accuracy: {accuracy}")
            with open("/mnt/data/test-accuracy", "w") as f:
              f.write(str(accuracy*100))
            EOF
        volumeMounts:
          - name: data-volume
            mountPath: /mnt/data

    # Step 4: Check accuracy, loop or upload
    - name: check-accuracy
      inputs:
        parameters:
          - name: test-accuracy
          - name: iteration
      dag:
        tasks:
          - name: upload-if-good
            template: store-model
            when: "{{inputs.parameters.test-accuracy}} >= 87"
          - name: retrain
            template: ml-workflow
            when: "{{inputs.parameters.test-accuracy}} < 87 && {{inputs.parameters.iteration}} < 5"
            arguments:
              parameters:
                - name: iteration
                  value: "{{=asInt(inputs.parameters.iteration) + 1}}"

    # Step 5: Store model in MinIO
    - name: store-model
      container:
        image: minio/mc
        envFrom:
          - secretRef:
              name: minio-secret
        command: ["sh", "-c"]
        args:
          - |
            echo "Uploading final model..."
            mc alias set myminio "$MINIO_SERVER_HOST" "$MINIO_SERVER_ACCESS_KEY" "$MINIO_SERVER_SECRET_KEY"
            mc cp /mnt/data/model.joblib myminio/yashod-ml/model.joblib
        volumeMounts:
          - name: data-volume
            mountPath: /mnt/data
